[
    {
        "question": "什么是决策树",
        "answer": "决策树是一种用于分类和回归分析的预测模型。它通过模拟人类做决策的过程，构建一个树状结构，其中每个内部节点表示一个特征或属性测试，外部节点代表最终的分类结果或回归预测值。决策树易于理解和解释，但也容易过拟合数据。\n",
        "source_documents": [
            {
                "page_content": "18.2\n决策树模型的可解释性\n我们首先介绍一下一个比较简单的机器学习模型，其在设计之初就已经有了比较好的可\n解释性，这个模型就是决策树模型。决策树相较于线性的模型，它是更强大的模型。而决策树\n的另外一个好处，\n相较于深度学习它具有良好的可解释性。\n比如从决策树的结构，\n我们就可以\n知道模型是凭借着什么样的规则来做出最终的判断。所以我们希望从决策树模型进行可解释\n性的研究，再扩展到其他机器学习模型，甚至深度模型。\n我们首先简单介绍一下决策树，\n它有很多的节点，\n那每一个节点都会问一个问题，\n让你决\n定向左还是向右。最终当你走到节点的末尾，即叶子节点的时候，就可以做出最终的决定。因\n为在每一个节点都有一个问题，我们看那些问题以及答案就可以知道现在整个模型凭借着什\n么样的特征如何做出最终的决断。所以从这个角度看来，决策树它既强大又有良好的可解释\n性。那我们是不是就可以用决策树来解决所有的问题呢？其实不是的，它是一个树状的结构，\n那我们可以想像一下，如果特征非常地多，得到决策树就会非常地复杂，就很难去解释它了。\n因为其节点太多而且很难分析得到整个模型的规则。所以复杂的决策树也有可能是一个黑盒\n子，它也有可能是一个非常地复杂的模型，所以我们也不能够一味地去使用决策树。\n另外一方面，\n我们是怎么实际使用决策树这个技术的呢？\n很多同学都会说，\n这个打Kaggle\n比赛的时候，深度学习不是最好用的，决策树才是最好用的，决策树才是Kaggle 比赛的常胜\n将军。但是其实当你在使用决策树的时候，并不是只用一棵决策树，你真正用的技术叫做随\n机森林。真正用的技术其实是好多棵决策树共同决定的结果。一棵决策树可以凭借着每一个\n节点的问题和答案知道它是怎么做出最终的判断的，但当你有一片森林的时候，你就很难知\n道说这一片森林是怎么做出最终的判断的。所以决策树也不是最终的答案，并不是有决策树，\n我们就解决了可解释性机器学习的问题。\n300→_→\nhttps://github.com/datawhalechina/leedl-tutorial\n←_←\n18.3\n可解释性机器学习的目标",
                "metadata": {
                    "chapter": "[18, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/LeeDL_Tutorial_v.1.1.4.pdf",
                    "title": "决策树模型的可解释性"
                }
            },
            {
                "page_content": "4.1\n基本流程\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行）\n，即“if⋯⋯elif⋯⋯\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜）\n，那么此时划分的目的\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性）\n，\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”\n，图4.2 第14 行的A\\{a∗}\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\n本（例如仍然同时含有好瓜和坏瓜）\n，但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\n归返回的情形(2)；\n（3）\n根据某个属性进行划分时，\n若该属性多个属性值中的某个属性值不包含任何样本\n（例如未收集到）\n，\n例如对当前子集以“纹理”属性来划分，\n“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\n能会有样本落入该分支。此即为递归返回的情形(3)。",
                "metadata": {
                    "chapter": "[4, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "基本流程"
                }
            },
            {
                "page_content": "4.2.5\n式(4.6) 的解释\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\n分属性的最优划分点。常用的CART 决策树的构造算法如下\n[1]：\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\nGini_index(D, a) = |Da=v|\n|D|\nGini(Da=v) + |Da̸=v|\n|D|\nGini(Da̸=v)\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\n(3) 重复以上两步，直至满足停止条件。\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\n6，反例占\np2 = 3\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\n5\n11，反例占\np2 =\n6\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\nGini_index(D, 色泽= 青绿)\n= 6\n17 ×\n�\n1 −\n�3\n6\n�2\n−\n�3\n6\n�2�\n+ 11\n17 ×\n�\n1 −\n�5\n11\n�2\n−\n�6\n11\n�2�\n= 0.497\n类似地，可以计算出不同属性取不同值的基尼指数如下：\nGini_index(D, 色泽= 乌黑) = 0.456\nGini_index(D, 色泽= 浅白) = 0.426\nGini_index(D, 根蒂= 蜷缩) = 0.456\nGini_index(D, 根蒂= 稍蜷) = 0.496\nGini_index(D, 根蒂= 硬挺) = 0.439\n→_→\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n←_←→_→\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\n←_←\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\nf(x) =\nM\n�\nm=1\ncmI(x ∈Rm)\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\n值作为该样本的预测值yi。",
                "metadata": {
                    "chapter": "[4, 2, 5]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "式(4.6)的解释"
                }
            },
            {
                "page_content": "第4 章\n决策树\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\n算法会更为通俗易懂。",
                "metadata": {
                    "chapter": "[4]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "决策树"
                }
            }
        ]
    },
    {
        "question": "决策树和支持向量机有什么不同，如何选择",
        "answer": "决策树和支持向量机（SVM）是两种不同的机器学习模型。\n\n**决策树**：\n- 是一种基于规则的学习方法，通过不断划分数据集，形成一个树状结构。\n- 非常直观，易于理解和解释。尤其适用于处理离散特征和分类任务。\n- 但是决策树容易过拟合，且对于连续性和高维数据的处理效果不佳。\n\n**支持向量机**：\n- SVM是一种监督学习模型，用于二分类或多分类问题。\n- SVM的核心思想是找到一个最优超平面（即最大间隔），将不同类别的样本分开。\n- SVM具有很好的泛化能力，在小样本和非线性可分数据集上有优势。\n- 但是SVM对于大规模数据的处理效率不高，而且调整参数比较困难。\n\n选择决策树还是支持向量机取决于具体的问题：\n- 如果问题是分类任务且需要易于理解和解释的结果，那么决策树是一个不错的选择。\n- 如果问题的数据具有较高的维度或复杂性，并且需要较好的泛化能力来应对未知数据，那么SVM可能会表现更好，特别是对于非线性可分数据集。\n\n\nassistant0\n",
        "source_documents": [
            {
                "page_content": "18.2\n决策树模型的可解释性\n我们首先介绍一下一个比较简单的机器学习模型，其在设计之初就已经有了比较好的可\n解释性，这个模型就是决策树模型。决策树相较于线性的模型，它是更强大的模型。而决策树\n的另外一个好处，\n相较于深度学习它具有良好的可解释性。\n比如从决策树的结构，\n我们就可以\n知道模型是凭借着什么样的规则来做出最终的判断。所以我们希望从决策树模型进行可解释\n性的研究，再扩展到其他机器学习模型，甚至深度模型。\n我们首先简单介绍一下决策树，\n它有很多的节点，\n那每一个节点都会问一个问题，\n让你决\n定向左还是向右。最终当你走到节点的末尾，即叶子节点的时候，就可以做出最终的决定。因\n为在每一个节点都有一个问题，我们看那些问题以及答案就可以知道现在整个模型凭借着什\n么样的特征如何做出最终的决断。所以从这个角度看来，决策树它既强大又有良好的可解释\n性。那我们是不是就可以用决策树来解决所有的问题呢？其实不是的，它是一个树状的结构，\n那我们可以想像一下，如果特征非常地多，得到决策树就会非常地复杂，就很难去解释它了。\n因为其节点太多而且很难分析得到整个模型的规则。所以复杂的决策树也有可能是一个黑盒\n子，它也有可能是一个非常地复杂的模型，所以我们也不能够一味地去使用决策树。\n另外一方面，\n我们是怎么实际使用决策树这个技术的呢？\n很多同学都会说，\n这个打Kaggle\n比赛的时候，深度学习不是最好用的，决策树才是最好用的，决策树才是Kaggle 比赛的常胜\n将军。但是其实当你在使用决策树的时候，并不是只用一棵决策树，你真正用的技术叫做随\n机森林。真正用的技术其实是好多棵决策树共同决定的结果。一棵决策树可以凭借着每一个\n节点的问题和答案知道它是怎么做出最终的判断的，但当你有一片森林的时候，你就很难知\n道说这一片森林是怎么做出最终的判断的。所以决策树也不是最终的答案，并不是有决策树，\n我们就解决了可解释性机器学习的问题。\n300→_→\nhttps://github.com/datawhalechina/leedl-tutorial\n←_←\n18.3\n可解释性机器学习的目标",
                "metadata": {
                    "chapter": "[18, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/LeeDL_Tutorial_v.1.1.4.pdf",
                    "title": "决策树模型的可解释性"
                }
            },
            {
                "page_content": "4.1\n基本流程\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行）\n，即“if⋯⋯elif⋯⋯\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜）\n，那么此时划分的目的\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性）\n，\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”\n，图4.2 第14 行的A\\{a∗}\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\n本（例如仍然同时含有好瓜和坏瓜）\n，但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\n归返回的情形(2)；\n（3）\n根据某个属性进行划分时，\n若该属性多个属性值中的某个属性值不包含任何样本\n（例如未收集到）\n，\n例如对当前子集以“纹理”属性来划分，\n“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\n能会有样本落入该分支。此即为递归返回的情形(3)。",
                "metadata": {
                    "chapter": "[4, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "基本流程"
                }
            },
            {
                "page_content": "4.2.5\n式(4.6) 的解释\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\n分属性的最优划分点。常用的CART 决策树的构造算法如下\n[1]：\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\nGini_index(D, a) = |Da=v|\n|D|\nGini(Da=v) + |Da̸=v|\n|D|\nGini(Da̸=v)\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\n(3) 重复以上两步，直至满足停止条件。\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\n6，反例占\np2 = 3\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\n5\n11，反例占\np2 =\n6\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\nGini_index(D, 色泽= 青绿)\n= 6\n17 ×\n�\n1 −\n�3\n6\n�2\n−\n�3\n6\n�2�\n+ 11\n17 ×\n�\n1 −\n�5\n11\n�2\n−\n�6\n11\n�2�\n= 0.497\n类似地，可以计算出不同属性取不同值的基尼指数如下：\nGini_index(D, 色泽= 乌黑) = 0.456\nGini_index(D, 色泽= 浅白) = 0.426\nGini_index(D, 根蒂= 蜷缩) = 0.456\nGini_index(D, 根蒂= 稍蜷) = 0.496\nGini_index(D, 根蒂= 硬挺) = 0.439\n→_→\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n←_←→_→\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\n←_←\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\nf(x) =\nM\n�\nm=1\ncmI(x ∈Rm)\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\n值作为该样本的预测值yi。",
                "metadata": {
                    "chapter": "[4, 2, 5]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "式(4.6)的解释"
                }
            },
            {
                "page_content": "第4 章\n决策树\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\n算法会更为通俗易懂。",
                "metadata": {
                    "chapter": "[4]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "决策树"
                }
            }
        ]
    }
]