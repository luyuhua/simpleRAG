[
    {
        "question": "什么是决策树",
        "answer": "决策树是一种用于分类和回归分析的监督学习方法。它通过构建一个树形结构来模拟决策过程。\n\n决策树的工作原理如下：\n1. **数据集**：首先需要有一个包含特征和标签的数据集。\n2. **选择节点**：从根节点开始，根据当前节点划分的最佳标准（如信息增益、基尼指数等）选择最佳的特征进行划分。\n3. **递归过程**：对每个子节点重复上述步骤，直到所有数据都被分类或达到预定的停止条件（例如达到最大树深度或者数据集中样本纯度达到一定程度）。\n4. **预测**：新的数据实例通过决策树的路径进行判断，从而得到其所属的类别。\n\n决策树因其易于理解和解释、可以处理非数值特征以及适用于各种规模的问题而受到广泛应用。\n",
        "source_documents": [
            {
                "page_content": "18.2\n决策树模型的可解释性\n我们首先介绍一下一个比较简单的机器学习模型，其在设计之初就已经有了比较好的可\n解释性，这个模型就是决策树模型。决策树相较于线性的模型，它是更强大的模型。而决策树\n的另外一个好处，\n相较于深度学习它具有良好的可解释性。\n比如从决策树的结构，\n我们就可以\n知道模型是凭借着什么样的规则来做出最终的判断。所以我们希望从决策树模型进行可解释\n性的研究，再扩展到其他机器学习模型，甚至深度模型。\n我们首先简单介绍一下决策树，\n它有很多的节点，\n那每一个节点都会问一个问题，\n让你决\n定向左还是向右。最终当你走到节点的末尾，即叶子节点的时候，就可以做出最终的决定。因\n为在每一个节点都有一个问题，我们看那些问题以及答案就可以知道现在整个模型凭借着什\n么样的特征如何做出最终的决断。所以从这个角度看来，决策树它既强大又有良好的可解释\n性。那我们是不是就可以用决策树来解决所有的问题呢？其实不是的，它是一个树状的结构，\n那我们可以想像一下，如果特征非常地多，得到决策树就会非常地复杂，就很难去解释它了。\n因为其节点太多而且很难分析得到整个模型的规则。所以复杂的决策树也有可能是一个黑盒\n子，它也有可能是一个非常地复杂的模型，所以我们也不能够一味地去使用决策树。\n另外一方面，\n我们是怎么实际使用决策树这个技术的呢？\n很多同学都会说，\n这个打Kaggle\n比赛的时候，深度学习不是最好用的，决策树才是最好用的，决策树才是Kaggle 比赛的常胜\n将军。但是其实当你在使用决策树的时候，并不是只用一棵决策树，你真正用的技术叫做随\n机森林。真正用的技术其实是好多棵决策树共同决定的结果。一棵决策树可以凭借着每一个\n节点的问题和答案知道它是怎么做出最终的判断的，但当你有一片森林的时候，你就很难知\n道说这一片森林是怎么做出最终的判断的。所以决策树也不是最终的答案，并不是有决策树，\n我们就解决了可解释性机器学习的问题。\n300→_→\nhttps://github.com/datawhalechina/leedl-tutorial\n←_←\n18.3\n可解释性机器学习的目标",
                "metadata": {
                    "chapter": "[18, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/LeeDL_Tutorial_v.1.1.4.pdf",
                    "title": "决策树模型的可解释性"
                }
            },
            {
                "page_content": "4.1\n基本流程\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行）\n，即“if⋯⋯elif⋯⋯\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜）\n，那么此时划分的目的\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性）\n，\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”\n，图4.2 第14 行的A\\{a∗}\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\n本（例如仍然同时含有好瓜和坏瓜）\n，但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\n归返回的情形(2)；\n（3）\n根据某个属性进行划分时，\n若该属性多个属性值中的某个属性值不包含任何样本\n（例如未收集到）\n，\n例如对当前子集以“纹理”属性来划分，\n“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\n能会有样本落入该分支。此即为递归返回的情形(3)。",
                "metadata": {
                    "chapter": "[4, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "基本流程"
                }
            },
            {
                "page_content": "4.2.5\n式(4.6) 的解释\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\n分属性的最优划分点。常用的CART 决策树的构造算法如下\n[1]：\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\nGini_index(D, a) = |Da=v|\n|D|\nGini(Da=v) + |Da̸=v|\n|D|\nGini(Da̸=v)\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\n(3) 重复以上两步，直至满足停止条件。\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\n6，反例占\np2 = 3\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\n5\n11，反例占\np2 =\n6\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\nGini_index(D, 色泽= 青绿)\n= 6\n17 ×\n�\n1 −\n�3\n6\n�2\n−\n�3\n6\n�2�\n+ 11\n17 ×\n�\n1 −\n�5\n11\n�2\n−\n�6\n11\n�2�\n= 0.497\n类似地，可以计算出不同属性取不同值的基尼指数如下：\nGini_index(D, 色泽= 乌黑) = 0.456\nGini_index(D, 色泽= 浅白) = 0.426\nGini_index(D, 根蒂= 蜷缩) = 0.456\nGini_index(D, 根蒂= 稍蜷) = 0.496\nGini_index(D, 根蒂= 硬挺) = 0.439\n→_→\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n←_←→_→\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\n←_←\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\nf(x) =\nM\n�\nm=1\ncmI(x ∈Rm)\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\n值作为该样本的预测值yi。",
                "metadata": {
                    "chapter": "[4, 2, 5]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "式(4.6)的解释"
                }
            },
            {
                "page_content": "第4 章\n决策树\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\n算法会更为通俗易懂。",
                "metadata": {
                    "chapter": "[4]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "决策树"
                }
            },
            {
                "page_content": "8.3.3\n随机森林的解释\n在8.3.2 节开篇第一句话就解释了随机森林的概念：随机森林是Bagging 的一个扩展变体，是以决策\n树为基学习器构建Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。\n完整版随机森林当然更复杂，这时只须知道两个重点：(1) 以决策树为基学习器；(2) 在基学习器训练\n过程中，选择划分属性时只使用当前结点属性集合的一个子集。",
                "metadata": {
                    "chapter": "[8, 3, 3]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "随机森林的解释"
                }
            },
            {
                "page_content": "1.2\n序列决策",
                "metadata": {
                    "chapter": "[1, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "序列决策"
                }
            },
            {
                "page_content": "11.1\n子集搜索与评价\n开篇给出了“特征选择”的概念, 并谈到特征选择与第10 章的降维有相似的动机。特征选择与降维的\n区别在于特征选择是从所有特征中简单地选出相关特征, 选择出来的特征就是原来的特征; 降维则对原来\n的特征进行了映射变换, 降维后的特征均不再是原来的特征。\n本节涉及“子集评价”的式(14.1) 和式(14.2) 与第4 章的式(4.2) 和式(4.1) 相同, 这是因为“决策\n树算法在构建树的同时也可看作进行了特征选择”(参见“11.7 阅读材料”)。接下来在11.2 节、11.3 节、\n11.4 节分别介绍的三类特征选择方法: 过滤式(filter)、包裹式(wrapper) 和嵌入式(embedding)。",
                "metadata": {
                    "chapter": "[11, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "子集搜索与评价"
                }
            },
            {
                "page_content": "7.1\n贝叶斯决策论",
                "metadata": {
                    "chapter": "[7, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "贝叶斯决策论"
                }
            },
            {
                "page_content": "4.2\n划分选择\n本节介绍的三种划分选择方法，即信息增益、增益率、基尼指数分别对应著名的ID3、C4.5 和CART\n三种决策树算法。",
                "metadata": {
                    "chapter": "[4, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "划分选择"
                }
            },
            {
                "page_content": "4.3\n剪枝处理\n本节内容通俗易懂，跟着“西瓜书”中的例子动手演算即可，无需做过多解释。以下仅结合图4.5 继\n续讨论一下图4.2 中的递归返回条件。图4.5 与图4.4 均是基于信息增益生成的决策树，不同在于图4.4\n基于表4.1，而图4.5 基于表4.2 的训练集。\n结点3 包含训练集“脐部”为稍凹的样本（编号6、7、15、17）\n，当根据“根蒂”再次进行划分时不\n含有“根蒂”为硬挺的样本（递归返回情形(3)）\n，而恰巧四个样本（编号6、7、15、17）含两个好瓜和两\n个坏瓜，因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。\n结点5 包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本（编号6、7、15）\n，当根据“色泽”再次\n进行划分时不含有“色泽”为浅白的样本（递归返回情形(3)）\n，因此叶结点浅白类别标记为好瓜（编号6、\n7、15 样本中，前两个为好瓜，最后一个为坏瓜）\n。\n结点6 包含训练集“脐部”为稍凹、\n“根蒂”为稍蜷、\n“色泽”为乌黑的样本（编号7、15）\n，当根据“纹\n理”再次进行划分时不含有“纹理”为模糊的样本（递归返回情形(3)）\n，而恰巧两个样本（编号7、15）含\n好瓜和坏瓜各一个，因此叶结点模糊的类别随机从类别好瓜和坏瓜中选择其一。\n图4.5 两次随机选择均选为好瓜，实际上表示了一种归纳偏好（参见第1 章1.4 节）\n。",
                "metadata": {
                    "chapter": "[4, 3]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "剪枝处理"
                }
            },
            {
                "page_content": "2.3.7\n预测与控制\n预测（prediction）和控制（control）是马尔可夫决策过程里面的核心问题。\n预测（评估一个给定的策略）的输入是马尔可夫决策过程< S, A, P, R, γ > 和策略π，输出是价值函\n数Vπ。预测是指给定一个马尔可夫决策过程以及一个策略π ，计算它的价值函数，也就是计算每个状态\n的价值。\n控制（搜索最佳策略）的输入是马尔可夫决策过程< S, A, P, R, γ >，输出是最佳价值函数（optimal\nvalue function）V ∗和最佳策略（optimal policy）π∗。控制就是我们去寻找一个最佳的策略，然后同时输\n出它的最佳价值函数以及最佳策略。\n在马尔可夫决策过程里面，预测和控制都可以通过动态规划解决。要强调的是，这两者的区别就在于，\n预测问题是给定一个策略，我们要确定它的价值函数是多少。而控制问题是在没有策略的前提下，我们要\n确定最佳的价值函数以及对应的决策方案。实际上，这两者是递进的关系，在强化学习中，我们通过解决\n预测问题，进而解决控制问题。\n举一个例子来说明预测与控制的区别。首先是预测问题。在图2.16a 的方格中，智能体可以采取上、\n下、左、右4 个动作。如果采取的动作让智能体走出网格，则其会在原位置不动，并且得到-1 的奖励。除\n了将智能体从A 和B 移走的动作外，其他动作的奖励均为0。智能体在A 采取任意一个动作，都会移动\n到A′ ，并且得到+10 的奖励。智能体在B 采取任意一个动作，都会移动到B′ ，并且得到+5 的奖励。\n如图2.16b 所示，现在，我们给定一个策略：在任何状态中，智能体的动作模式都是随机的，也就是上、\n下、左、右的概率均为0.25。预测问题要做的就是，求出在这种决策模式下的价值函数。图2.16c 是折扣\n因子为γ = 0.9 时对应的价值函数。\n接着是控制问题。在控制问题中，问题背景与预测问题的相同，唯一的区别就是：不再限制策略。也\n就是动作模式是未知的，我们需要自己确定。所以我们通过解决控制问题，求得每一个状态的最优的价值\n函数，如图2.17b 所示；也得到了最优的策略，如图2.17c 所示。控制问题要做的就是，给定同样的条件，\n求出在所有可能的策略下最优的价值函数是什么，最优策略是什么。\n34→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n(a) 特殊情况下的跳转及其对应的奖励\n(b) 动作\n(c) 等概率随机策略下的价值函数\n图2.16\n网格世界例子：预测[3]\n(a) 特殊情况下的跳转及其对应\n的奖励\n(b) v∗\n(c) π∗\n图2.17\n网格世界例子：控制\n2.3.8\n动态规划",
                "metadata": {
                    "chapter": "[2, 3, 7]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "预测与控制"
                }
            },
            {
                "page_content": "1.4.3\n模型\n第3 个组成部分是模型，模型决定了下一步的状态。下一步的状态取决于当前的状态以及当前采取的\n动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即\npa\nss′ = p (st+1 = s′ | st = s, at = a)\n(1.6)\n奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即\nR(s, a) = E [rt+1 | st = s, at = a]\n(1.7)\n当我们有了策略、价值函数和模型3 个组成部分后，就形成了一个马尔可夫决策过程（Markov de-\ncision process）\n。如图1.15 所示，这个决策过程可视化了状态之间的转移以及采取的动作。\n图1.15\n马尔可夫决策过程\n我们来看一个走迷宫的例子。如图1.16 所示，要求智能体从起点（start）开始，然后到达终点（goal）\n的位置。每走一步，我们就会得到−1 的奖励。我们可以采取的动作是往上、下、左、右走。我们用现在\n智能体所在的位置来描述当前状态。\n图1.16\n走迷宫的例子\n我们可以用不同的强化学习方法来解这个环境。\n如果我们采取基于策略的强化学习\n（policy-based RL）\n方法，当学习好了这个环境后，在每一个状态，我们都会得到一个最佳的动作。如图1.17 所示，比如我们\n现在在起点位置，我们知道最佳动作是往右走；在第二格的时候，得到的最佳动作是往上走；第三格是往\n右走...... 通过最佳的策略，我们可以最快地到达终点。\n如果换成基于价值的强化学习（value-based RL）方法，利用价值函数作为导向，我们就会得到另外\n一种表征，每一个状态会返回一个价值。如图1.18 所示，比如我们在起点位置的时候，价值是−16，因为\n我们最快可以16 步到达终点。因为每走一步会减1，所以这里的价值是−16。当我们快接近终点的时候，\n10→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n图1.17\n使用基于策略的强化学习方法得到的结果\n这个数字变得越来越大。在拐角的时候，比如现在在第二格，价值是−15，智能体会看上、下两格，它看\n到上面格子的价值变大了，变成−14 了，下面格子的价值是−16，那么智能体就会采取一个往上走的动\n作。所以通过学习的价值的不同，我们可以抽取出现在最佳的策略。\n图1.18\n使用基于价值的强化学习方法得到的结果",
                "metadata": {
                    "chapter": "[1, 4, 3]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "模型"
                }
            },
            {
                "page_content": "8.5.14\n多样性增强的解释\n在8.5.3 节介绍了四种多样性增强的方法, 通俗易懂, 几乎不需要什么注解, 仅强调几个概念:\n(1) 数据样本扰动中提到了“不稳定基学习器”(例如决策树、神经网络等) 和“稳定基学习器”(例\n如线性学习器、支持向量机、朴素贝叶斯、k 近邻学习器等), 对稳定基学习器进行集成时数据样本扰动技\n巧效果有限。这也就可以解释为什么随机森林和GBDT 等以决策树为基分学习器的集成方法很成功吧,\nGradient Boosting 和Bagging 都是以数据样本扰动来增强多样性的; 而且, 掌握这个经验后在实际工程应\n用中就可以排除一些候选基分类器, 但论文中的确经常见到以支持向量机为基分类器Bagging 实现, 这可\n能是由于LIBSVM 简单易用的原因吧。\n(2)“西瓜书”图8.11 随机子空间算法, 针对每个基分类器ht 在训练时使用了原数据集的部分输入属\n性（末必是初始属性, 详见第189 页左上注释), 因此在最终集成时“西瓜书”图8.11 最后一行也要使用\n相同的部分属性。\n(3) 输出表示扰动中提到了“翻转法”(Flipping Output), 看起来是一个并没有道理的技巧, 为什么要\n将训练样本的标记改变呢? 若认为原训练样本标记是完全可靠的, 这不是人为地加入噪声么? 但西瓜书作\n者2017 年提出的深度森林[3] 模型中也用到了该技巧, 正如本小节名为“多样性增强”, 虽然从局部来看\n引入了标记噪声, 但从模型集成的角度来说却是有益的。",
                "metadata": {
                    "chapter": "[8, 5, 14]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "多样性增强的解释"
                }
            },
            {
                "page_content": "3.1\n马尔可夫决策过程\n强化学习是一个与时间相关的序列决策的问题。例如，如图3.1 所示，在t −1 时刻，我看到熊对我\n招手，下意识的动作就是逃跑。熊看到有人逃跑，就可能觉得发现了猎物，并开始发动攻击。而在t 时刻，\n我如果选择装死的动作，可能熊咬咬我、摔几下就觉得挺无趣的，可能会走开。这个时候我再逃跑，可能\n就成功了，这就是一个序列决策过程。\n在输出每一个动作之前，我们可以选择不同的动作。比如在t 时刻，我选择逃跑的时候，可能熊已\n经追上来了。如果在t 时刻，我没有选择装死，而是选择逃跑，这个时候熊已经追上来了，那么我就会\n转移到不同的状态。有一定的概率我会逃跑成功，也有一定的概率我会逃跑失败。我们用状态转移概率\np [st+1, rt | st, at] 来表示在状态st 选择动作at 的时候，转移到转态st+1 ，而且得到奖励rt 的概率是多\n少。状态转移概率是具有马尔可夫性质的（系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往\n任何状态）\n。因为在这个过程中，下一时刻的状态取决于当前的状态st，它和之前的st−1 和st−2 没有关\n系。再加上这个过程也取决于智能体与环境交互的at ，所以包含了决策的过程，我们称这样的过程为马\n尔可夫决策过程。马尔可夫决策过程就是序列决策的经典的表现方式。马尔可夫决策过程也是强化学习里\n面一个非常基本的学习框架。状态、动作、状态转移概率和奖励(S、A、P、R)，这4 个合集就构成了强\n化学习马尔可夫决策过程的四元组，后面也可能会再加上折扣因子构成五元组。\n图3.1\n马尔可夫决策过程四元组",
                "metadata": {
                    "chapter": "[3, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "马尔可夫决策过程"
                }
            },
            {
                "page_content": "4.5\n多变量决策树\n本节内容也通俗易懂，以下仅对部分图做进一步解释说明。",
                "metadata": {
                    "chapter": "[4, 5]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "多变量决策树"
                }
            },
            {
                "page_content": "2.3.14\n马尔可夫决策过程中的预测和控制总结\n总结如表2.1 所示，我们使用动态规划算法来解马尔可夫决策过程里面的预测和控制，并且采取不同\n的贝尔曼方程。对于预测问题，即策略评估的问题，我们不停地执行贝尔曼期望方程，这样就可以估计出\n43→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n(a) 第二次策略评估\n(b) 第二次策略更新\n图2.26\n马尔可夫决策过程控制：策略迭代示例\n(a) 第3 次策略评估\n(b) 第3 次策略更新\n图2.27\n马尔可夫决策过程控制：策略迭代示例\n给定的策略，然后得到价值函数。对于控制问题，如果我们采取的算法是策略迭代，使用的就是贝尔曼期\n望方程；如果我们采取的算法是价值迭代，使用的就是贝尔曼最优方程。",
                "metadata": {
                    "chapter": "[2, 3, 14]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "马尔可夫决策过程中的预测和控制总结"
                }
            },
            {
                "page_content": "第2 章\n马尔可夫决策过程\n图2.1 介绍了强化学习里面智能体与环境之间的交互，智能体得到环境的状态后，它会采取动作，并\n把这个采取的动作返还给环境。环境得到智能体的动作后，它会进入下一个状态，把下一个状态传给智能\n体。在强化学习中，智能体与环境就是这样进行交互的，这个交互过程可以通过马尔可夫决策过程来表示，\n所以马尔可夫决策过程是强化学习的基本框架。\n图2.1\n智能体与环境之间的交互\n本章将介绍马尔可夫决策过程。在介绍马尔可夫决策过程之前，我们先介绍它的简化版本：马尔可夫\n过程（Markov process，MP）以及马尔可夫奖励过程（Markov reward process，MRP）\n。通过与这两种\n过程的比较，我们可以更容易理解马尔可夫决策过程。其次，我们会介绍马尔可夫决策过程中的策略评估\n（policy evaluation）\n，就是当给定决策后，我们怎么去计算它的价值函数。最后，我们会介绍马尔可夫决\n策过程的控制，具体有策略迭代（policy iteration）和价值迭代（value iteration）两种算法。在马尔\n可夫决策过程中，它的环境是全部可观测的。但是很多时候环境里面有些量是不可观测的，但是这个部分\n观测的问题也可以转换成马尔可夫决策过程的问题。",
                "metadata": {
                    "chapter": "[2]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "马尔可夫决策过程"
                }
            },
            {
                "page_content": "8.1\n个体与集成\n基学习器(base learner) 的概念在论文中经常出现，\n可留意一下；\n另外，\n本节提到的投票法有两种，\n除\n了本节的多数投票(majority voting)，\n还有概率投票(probability voting)，\n这两点在8.4 节中均会提及，\n即\n硬投票和软投票。",
                "metadata": {
                    "chapter": "[8, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "个体与集成"
                }
            },
            {
                "page_content": "2.3.6\n策略评估\n已知马尔可夫决策过程以及要采取的策略π ，计算价值函数Vπ(s) 的过程就是策略评估。策略评估在\n有些地方也被称为（价值）预测[（value）prediction）]，也就是预测我们当前采取的策略最终会产生多\n少价值。如图2.14a 所示，对于马尔可夫决策过程，我们其实可以把它想象成一个摆渡的人在船上，她可\n以控制船的移动，\n避免船随波逐流。\n因为在每一个时刻，\n摆渡的人采取的动作会决定船的方向。\n如图2.14b\n所示，对于马尔可夫奖励过程与马尔可夫过程，纸的小船会随波逐流，然后产生轨迹。马尔可夫决策过程\n的不同之处在于有一个智能体控制船，这样我们就可以尽可能多地获得奖励。\n(a) 马尔可夫决策过程：人为控制船\n(b) 马尔可夫过程/马尔可夫奖励过程：随波逐流\n图2.14\n马尔可夫决策过程与马尔可夫过程/马尔可夫奖励过程的区别\n我们再看一下策略评估的例子，探究怎么在决策过程中计算每一个状态的价值。如图2.15 所示，假\n设环境里面有两种动作：往左走和往右走。现在的奖励函数应该是关于动作和状态两个变量的函数。但这\n里规定，不管智能体采取什么动作，只要到达状态s1，就有5 的奖励；只要到达状态s7 ，就有10 的奖\n励，到达其他状态没有奖励。我们可以将奖励函数表示为R = [5, 0, 0, 0, 0, 0, 10]。假设智能体现在采取一\n个策略：不管在任何状态，智能体采取的动作都是往左走，即采取的是确定性策略π(s) = 左。假设价值\n折扣因子γ = 0，那么对于确定性策略，最后估算出的价值函数是一致的，即V π = [5, 0, 0, 0, 0, 0, 10]。\n33→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n图2.15\n策略评估示例\n我们可以直接通过贝尔曼方程来得到价值函数：\nV k\nπ (s) = r(s, π(s)) + γ\n�\ns′∈S\np (s′ | s, π(s)) V k−1\nπ\n(s′)\n(2.37)\n其中，k 是迭代次数。我们可以不停地迭代，最后价值函数会收敛。收敛之后，价值函数的值就是每一个\n状态的价值。\n再来看一个例子，如果折扣因子γ = 0.5，我们可以通过式(2.38) 进行迭代：\nV t\nπ(s) =\n�\na\np(π(s) = a)\n�\nr(s, a) + γ\n�\ns′∈S\np (s′ | s, a) V t−1\nπ\n(s′)\n�\n(2.38)\n其中，t 是迭代次数。然后就可以得到它的状态价值。\n最后，例如，我们现在采取随机策略，在每个状态下，有0.5 的概率往左走，有0.5 的概率往右走，即\np(π(s) = 左) = 0.5，p(π(s) = 右) = 0.5，如何求出这个策略下的状态价值呢？我们可以这样做：一开始的\n时候，我们对V (s′) 进行初始化，不同的V (s′) 都会有一个值；接着，我们将V (s′) 代入贝尔曼期望方程\n里面进行迭代，就可以算出它的状态价值。",
                "metadata": {
                    "chapter": "[2, 3, 6]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "策略评估"
                }
            },
            {
                "page_content": "2.3.9\n马尔可夫决策过程中的策略评估\n策略评估就是给定马尔可夫决策过程和策略，评估我们可以获得多少价值，即对于当前策略，我们可\n以得到多大的价值。我们可以直接把贝尔曼期望备份（Bellman expectation backup）\n，变成迭代的过\n程，反复迭代直到收敛。这个迭代过程可以看作同步备份（synchronous backup）的过程。\n同步备份是指每一次的迭代都会完全更新所有的状态，这对于程序资源的需求特别大。异步备份\n（asynchronous backup）的思想就是通过某种方式，使得每一次迭代不需要更新所有的状态，因为\n事实上，很多状态也不需要被更新。\n35→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n式(2.39) 是指我们可以把贝尔曼期望备份转换成动态规划的迭代。当我们得到上一时刻的Vt 的时候，\n就可以通过递推的关系推出下一时刻的值。反复迭代，最后V 的值就是从V1、V2 到最后收敛之后的值\nVπ。Vπ 就是当前给定的策略π 对应的价值函数。\nV t+1(s) =\nX\na∈A\nπ(a | s)\n \nR(s, a) + γ\nX\ns′∈S\np (s′ | s, a) V t (s′)\n!\n(2.39)\n策略评估的核心思想就是把如式(2.39) 所示的贝尔曼期望备份反复迭代，然后得到一个收敛的价值\n函数的值。因为已经给定了策略函数，所以我们可以直接把它简化成一个马尔可夫奖励过程的表达形式，\n相当于把a 去掉，即\nVt+1(s) = rπ(s) + γPπ (s′ | s) Vt (s′)\n(2.40)\n这样迭代的式子中就只有价值函数与状态转移函数了。通过迭代式(2.40)，我们也可以得到每个状态的价\n值。因为不管是在马尔可夫奖励过程，还是在马尔可夫决策过程中，价值函数V 包含的变量都是只与状\n态有关，其表示智能体进入某一个状态，未来可能得到多大的价值。\n比如现在的环境是一个小网格世界（small gridworld）\n，智能体的目的是从某一个状态开始行走，然后\n到达终止状态，它的终止状态就是左上角与右下角（如图2.18 （右）所示的阴影方块）\n。小网格世界总共\n有14 个非终止状态：1, · · · , 14。我们把它的每个位置用一个状态来表示。\n如图2.18 （左）所示，在小网格世界中，智能体的策略函数直接给定了，它在每一个状态都是随机\n行走，即在每一个状态都是上、下、左、右行走，采取均匀的随机策略（uniform random policy）\n，π(l |\n.) = π(r | .) = π(u | .) = π(d | .) = 0.25。它在边界状态的时候，比如在第4 号状态的时候往左走，依然\n留在第4 号状态。我们对其加了限制，这个限制就是出边界的动作不会改变状态，相应概率设置为1，如\np(7 | 7, r) = 1。我们给出的奖励函数就是智能体每走一步，就会得到−1 的奖励，也就是到达终止状态之\n前每走一步获得的奖励都是−1，所以智能体需要尽快地到达终止状态。\n给定动作之后状态之间的转移（transition）是确定的，例如p(2 | 6,u) = 1，即从第6 号状态往上走，\n它就会直接到达第2 号状态。很多时候有些环境是概率性的（probabilistic）\n，比如智能体在第6 号状态，\n它选择往上走的时候，地板可能是滑的，然后它可能滑到第3 号状态或者第1 号状态，这就是有概率的转\n移。但我们把环境进行了简化，从6 号状态往上走，它就到了第2 号状态。因为我们已经知道环境中的每\n一个概率以及概率转移，所以就可以直接使用式(2.40) 进行迭代，这样就会算出每一个状态的价值。\n(a) 小网格世界\n(b) 动作集\n图2.18\n小网格世界环境\n我们再来看一个动态的例子，推荐斯坦福大学的一个网页：https://cs.stanford.edu/people/kar\npathy/reinforcejs/gridworld_dp.html，这个网页模拟了式(2.39) 所示的单步更新的过程中，所有格\n子的状态价值的变化过程。\n如图2.19a 所示，网格世界里面有很多格子，每个格子都代表一个状态。每个格子里面有一个初始值\n0。每个格子里还有一个箭头，这个箭头是指智能体在当前状态应该采取什么策略。我们这里采取随机的\n36\n→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n已经有奖励的状态逐渐扩散。当我们执行很多次迭代之后，各个状态的值会逐渐稳定下来，最后值就会确\n定不变。收敛之后，每个状态的值就是它的状态价值。",
                "metadata": {
                    "chapter": "[2, 3, 9]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "马尔可夫决策过程中的策略评估"
                }
            }
        ]
    },
    {
        "question": "决策树和支持向量机有什么不同，如何选择",
        "answer": "决策树和支持向量机（SVM）是两种不同的机器学习方法。它们的不同点和适用场景如下：\n\n1. **决策树**：\n   - 是一种基于特征分类的监督学习算法。\n   - 通过构建一棵树状结构，每个内部节点代表一个属性测试，叶结点给出类别预测。\n   - 决策树易于理解和解释，但可能会过拟合。\n\n2. **支持向量机（SVM））**：\n   - 是一种基于间隔最大化的二类监督学习算法。\n   - SVM通过核函数将原始数据映射到高维空间，寻找最优的超平面作为分类边界。\n   - SVM具有较好的泛化能力，不易过拟合。但选择合适的核函数和参数调整可能比较复杂。\n\n如何选择：\n- **决策树**适合于解释性强、需要直观理解问题领域，如用户行为分析或医疗诊断等。\n- **支持向量机**适合于解决复杂的分类问题，尤其在特征维度高或者数据分布不均匀时具有优势。另外，SVM在小型数据集上也能表现出良好的性能。\n\n综上所述，在选择决策树和支持向量机时，应根据具体任务的特点、数据的规模和质量以及对模型解释性的要求等因素进行综合评估。\nly one\n",
        "source_documents": [
            {
                "page_content": "18.2\n决策树模型的可解释性\n我们首先介绍一下一个比较简单的机器学习模型，其在设计之初就已经有了比较好的可\n解释性，这个模型就是决策树模型。决策树相较于线性的模型，它是更强大的模型。而决策树\n的另外一个好处，\n相较于深度学习它具有良好的可解释性。\n比如从决策树的结构，\n我们就可以\n知道模型是凭借着什么样的规则来做出最终的判断。所以我们希望从决策树模型进行可解释\n性的研究，再扩展到其他机器学习模型，甚至深度模型。\n我们首先简单介绍一下决策树，\n它有很多的节点，\n那每一个节点都会问一个问题，\n让你决\n定向左还是向右。最终当你走到节点的末尾，即叶子节点的时候，就可以做出最终的决定。因\n为在每一个节点都有一个问题，我们看那些问题以及答案就可以知道现在整个模型凭借着什\n么样的特征如何做出最终的决断。所以从这个角度看来，决策树它既强大又有良好的可解释\n性。那我们是不是就可以用决策树来解决所有的问题呢？其实不是的，它是一个树状的结构，\n那我们可以想像一下，如果特征非常地多，得到决策树就会非常地复杂，就很难去解释它了。\n因为其节点太多而且很难分析得到整个模型的规则。所以复杂的决策树也有可能是一个黑盒\n子，它也有可能是一个非常地复杂的模型，所以我们也不能够一味地去使用决策树。\n另外一方面，\n我们是怎么实际使用决策树这个技术的呢？\n很多同学都会说，\n这个打Kaggle\n比赛的时候，深度学习不是最好用的，决策树才是最好用的，决策树才是Kaggle 比赛的常胜\n将军。但是其实当你在使用决策树的时候，并不是只用一棵决策树，你真正用的技术叫做随\n机森林。真正用的技术其实是好多棵决策树共同决定的结果。一棵决策树可以凭借着每一个\n节点的问题和答案知道它是怎么做出最终的判断的，但当你有一片森林的时候，你就很难知\n道说这一片森林是怎么做出最终的判断的。所以决策树也不是最终的答案，并不是有决策树，\n我们就解决了可解释性机器学习的问题。\n300→_→\nhttps://github.com/datawhalechina/leedl-tutorial\n←_←\n18.3\n可解释性机器学习的目标",
                "metadata": {
                    "chapter": "[18, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/LeeDL_Tutorial_v.1.1.4.pdf",
                    "title": "决策树模型的可解释性"
                }
            },
            {
                "page_content": "4.1\n基本流程\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行）\n，即“if⋯⋯elif⋯⋯\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜）\n，那么此时划分的目的\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性）\n，\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”\n，图4.2 第14 行的A\\{a∗}\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\n本（例如仍然同时含有好瓜和坏瓜）\n，但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\n归返回的情形(2)；\n（3）\n根据某个属性进行划分时，\n若该属性多个属性值中的某个属性值不包含任何样本\n（例如未收集到）\n，\n例如对当前子集以“纹理”属性来划分，\n“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\n能会有样本落入该分支。此即为递归返回的情形(3)。",
                "metadata": {
                    "chapter": "[4, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "基本流程"
                }
            },
            {
                "page_content": "4.2.5\n式(4.6) 的解释\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\n分属性的最优划分点。常用的CART 决策树的构造算法如下\n[1]：\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\nGini_index(D, a) = |Da=v|\n|D|\nGini(Da=v) + |Da̸=v|\n|D|\nGini(Da̸=v)\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\n(3) 重复以上两步，直至满足停止条件。\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\n6，反例占\np2 = 3\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\n5\n11，反例占\np2 =\n6\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\nGini_index(D, 色泽= 青绿)\n= 6\n17 ×\n�\n1 −\n�3\n6\n�2\n−\n�3\n6\n�2�\n+ 11\n17 ×\n�\n1 −\n�5\n11\n�2\n−\n�6\n11\n�2�\n= 0.497\n类似地，可以计算出不同属性取不同值的基尼指数如下：\nGini_index(D, 色泽= 乌黑) = 0.456\nGini_index(D, 色泽= 浅白) = 0.426\nGini_index(D, 根蒂= 蜷缩) = 0.456\nGini_index(D, 根蒂= 稍蜷) = 0.496\nGini_index(D, 根蒂= 硬挺) = 0.439\n→_→\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n←_←→_→\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\n←_←\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\nf(x) =\nM\n�\nm=1\ncmI(x ∈Rm)\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\n值作为该样本的预测值yi。",
                "metadata": {
                    "chapter": "[4, 2, 5]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "式(4.6)的解释"
                }
            },
            {
                "page_content": "第4 章\n决策树\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\n算法会更为通俗易懂。",
                "metadata": {
                    "chapter": "[4]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "决策树"
                }
            },
            {
                "page_content": "11.1\n子集搜索与评价\n开篇给出了“特征选择”的概念, 并谈到特征选择与第10 章的降维有相似的动机。特征选择与降维的\n区别在于特征选择是从所有特征中简单地选出相关特征, 选择出来的特征就是原来的特征; 降维则对原来\n的特征进行了映射变换, 降维后的特征均不再是原来的特征。\n本节涉及“子集评价”的式(14.1) 和式(14.2) 与第4 章的式(4.2) 和式(4.1) 相同, 这是因为“决策\n树算法在构建树的同时也可看作进行了特征选择”(参见“11.7 阅读材料”)。接下来在11.2 节、11.3 节、\n11.4 节分别介绍的三类特征选择方法: 过滤式(filter)、包裹式(wrapper) 和嵌入式(embedding)。",
                "metadata": {
                    "chapter": "[11, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "子集搜索与评价"
                }
            },
            {
                "page_content": "6.1\n间隔与支持向量",
                "metadata": {
                    "chapter": "[6, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "间隔与支持向量"
                }
            },
            {
                "page_content": "2.3.7\n预测与控制\n预测（prediction）和控制（control）是马尔可夫决策过程里面的核心问题。\n预测（评估一个给定的策略）的输入是马尔可夫决策过程< S, A, P, R, γ > 和策略π，输出是价值函\n数Vπ。预测是指给定一个马尔可夫决策过程以及一个策略π ，计算它的价值函数，也就是计算每个状态\n的价值。\n控制（搜索最佳策略）的输入是马尔可夫决策过程< S, A, P, R, γ >，输出是最佳价值函数（optimal\nvalue function）V ∗和最佳策略（optimal policy）π∗。控制就是我们去寻找一个最佳的策略，然后同时输\n出它的最佳价值函数以及最佳策略。\n在马尔可夫决策过程里面，预测和控制都可以通过动态规划解决。要强调的是，这两者的区别就在于，\n预测问题是给定一个策略，我们要确定它的价值函数是多少。而控制问题是在没有策略的前提下，我们要\n确定最佳的价值函数以及对应的决策方案。实际上，这两者是递进的关系，在强化学习中，我们通过解决\n预测问题，进而解决控制问题。\n举一个例子来说明预测与控制的区别。首先是预测问题。在图2.16a 的方格中，智能体可以采取上、\n下、左、右4 个动作。如果采取的动作让智能体走出网格，则其会在原位置不动，并且得到-1 的奖励。除\n了将智能体从A 和B 移走的动作外，其他动作的奖励均为0。智能体在A 采取任意一个动作，都会移动\n到A′ ，并且得到+10 的奖励。智能体在B 采取任意一个动作，都会移动到B′ ，并且得到+5 的奖励。\n如图2.16b 所示，现在，我们给定一个策略：在任何状态中，智能体的动作模式都是随机的，也就是上、\n下、左、右的概率均为0.25。预测问题要做的就是，求出在这种决策模式下的价值函数。图2.16c 是折扣\n因子为γ = 0.9 时对应的价值函数。\n接着是控制问题。在控制问题中，问题背景与预测问题的相同，唯一的区别就是：不再限制策略。也\n就是动作模式是未知的，我们需要自己确定。所以我们通过解决控制问题，求得每一个状态的最优的价值\n函数，如图2.17b 所示；也得到了最优的策略，如图2.17c 所示。控制问题要做的就是，给定同样的条件，\n求出在所有可能的策略下最优的价值函数是什么，最优策略是什么。\n34→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n(a) 特殊情况下的跳转及其对应的奖励\n(b) 动作\n(c) 等概率随机策略下的价值函数\n图2.16\n网格世界例子：预测[3]\n(a) 特殊情况下的跳转及其对应\n的奖励\n(b) v∗\n(c) π∗\n图2.17\n网格世界例子：控制\n2.3.8\n动态规划",
                "metadata": {
                    "chapter": "[2, 3, 7]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "预测与控制"
                }
            },
            {
                "page_content": "8.5.14\n多样性增强的解释\n在8.5.3 节介绍了四种多样性增强的方法, 通俗易懂, 几乎不需要什么注解, 仅强调几个概念:\n(1) 数据样本扰动中提到了“不稳定基学习器”(例如决策树、神经网络等) 和“稳定基学习器”(例\n如线性学习器、支持向量机、朴素贝叶斯、k 近邻学习器等), 对稳定基学习器进行集成时数据样本扰动技\n巧效果有限。这也就可以解释为什么随机森林和GBDT 等以决策树为基分学习器的集成方法很成功吧,\nGradient Boosting 和Bagging 都是以数据样本扰动来增强多样性的; 而且, 掌握这个经验后在实际工程应\n用中就可以排除一些候选基分类器, 但论文中的确经常见到以支持向量机为基分类器Bagging 实现, 这可\n能是由于LIBSVM 简单易用的原因吧。\n(2)“西瓜书”图8.11 随机子空间算法, 针对每个基分类器ht 在训练时使用了原数据集的部分输入属\n性（末必是初始属性, 详见第189 页左上注释), 因此在最终集成时“西瓜书”图8.11 最后一行也要使用\n相同的部分属性。\n(3) 输出表示扰动中提到了“翻转法”(Flipping Output), 看起来是一个并没有道理的技巧, 为什么要\n将训练样本的标记改变呢? 若认为原训练样本标记是完全可靠的, 这不是人为地加入噪声么? 但西瓜书作\n者2017 年提出的深度森林[3] 模型中也用到了该技巧, 正如本小节名为“多样性增强”, 虽然从局部来看\n引入了标记噪声, 但从模型集成的角度来说却是有益的。",
                "metadata": {
                    "chapter": "[8, 5, 14]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "多样性增强的解释"
                }
            },
            {
                "page_content": "6.5.1\n式(6.43) 的解释\n相比于线性回归用一条线来拟合训练样本，支持向量回归而是采用一个以f(x) = wTx + b 为中心，\n宽度为2ϵ 的间隔带，来拟合训练样本。\n落在带子上的样本不计算损失（类比线性回归在线上的点预测误差为0）\n，不在带子上的则以偏离带\n子的距离作为损失（类比线性回归的均方误差）\n，然后以最小化损失的方式迫使间隔带从样本最密集的地\n方穿过，进而达到拟合训练样本的目的。因此支持向量回归的优化问题可以写为\nmin\nw,b\n1\n2∥w∥2 + C\nm\n�\ni=1\nℓϵ (f (xi) −yi)\n其中ℓϵ(z) 为“ϵ 不敏感损失函数”\n（类比线性回归的均方误差损失）\nℓϵ(z) =\n�\n0,\nif |z| ⩽ϵ\n|z| −ϵ,\nif |z| > ϵ\n1\n2∥w∥2 为L2 正则项，此处引入正则项除了起正则化本身的作用外，也是为了和软间隔支持向量机的优化\n目标保持形式上的一致，这样就可以导出对偶问题引入核函数，C 为用来调节损失权重的正则化常数。",
                "metadata": {
                    "chapter": "[6, 5, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "式(6.43)的解释"
                }
            },
            {
                "page_content": "4.2\n划分选择\n本节介绍的三种划分选择方法，即信息增益、增益率、基尼指数分别对应著名的ID3、C4.5 和CART\n三种决策树算法。",
                "metadata": {
                    "chapter": "[4, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "划分选择"
                }
            },
            {
                "page_content": "6.5\n支持向量回归",
                "metadata": {
                    "chapter": "[6, 5]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "支持向量回归"
                }
            },
            {
                "page_content": "3.1.3\n有模型与免模型的区别\n如图3.4 所示，策略迭代和价值迭代都需要得到环境的转移和奖励函数，所以在这个过程中，智能体\n没有与环境进行交互。在很多实际的问题中，马尔可夫决策过程的模型有可能是未知的，也有可能因模型\n太大不能进行迭代的计算，比如雅达利游戏、围棋、控制直升飞机、股票交易等问题，这些问题的状态转\n移非常复杂。\n如图3.5 所示，当马尔可夫决策过程的模型未知或者模型很大时，我们可以使用免模型强化学习的方\n法。免模型强化学习方法没有获取环境的状态转移和奖励函数，而是让智能体与环境进行交互，采集大量\n的轨迹数据，智能体从轨迹中获取信息来改进策略，从而获得更多的奖励。\n48→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n图3.4\n有模型强化学习方法\n图3.5\n免模型强化学习方法\n3.2\nQ 表格",
                "metadata": {
                    "chapter": "[3, 1, 3]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "有模型与免模型的区别"
                }
            },
            {
                "page_content": "6.1\n输入是向量序列的情况\n在图像识别的时候，假设输入的图像大小都是一样的。但如果问题变得复杂，如图6.2 所\n示，\n输入是一组向量，\n并且输入的向量的数量是会改变的，\n即每次模型输入的序列长度都不一\n样，这个时候应该要怎么处理呢？我们通过具体的例子来讲解处理方法。\n图6.2\n输入是一组向量\n第一个例子是文字处理，\n假设网络的输入是一个句子，\n每一个句子的长度都不一样\n（每个\n句子里面词汇的数量都不一样）\n。如果把一个句子里面的每一个词汇都描述成一个向量，用向\n量来表示，\n模型的输入就是一个向量序列，\n而且该向量序列的大小每次都不一样\n（句子的长度\n不一样，向量序列的大小就不一样）\n。\n将词汇表示成向量最简单的做法是独热编码，创建一个很长的向量，该向量的长度跟世\n界上存在的词汇的数量是一样多的。假设英文是十万个词汇，创建一个十万维的向量，每一\n个维度对应到一个词汇，如式(6.1) 所示。但是这种表示方法有一个非常严重的问题，它假设\n所有的词汇彼此之间都是没有关系的。cat 和dog 都是动物，它们应该比较像；cat 是动物，\napple 是植物，它们应该比较不像。但从独热向量中不能看到这件事情，其里面没有任何语义\n的信息。\napple = [1, 0, 0, 0, 0, . . . . . . ]\nbag = [0, 1, 0, 0, 0, . . . . . . ]\ncat = [0, 0, 1, 0, 0, . . . . . . ]\ndog = [0, 0, 0, 1, 0, . . . . . . ]\nelephant = [0, 0, 0, 0, 1, . . . . . . ]\n(6.1)\n113→_→\nhttps://github.com/datawhalechina/leedl-tutorial\n←_←\n个人。\n每一个节点可以看作是一个向量。\n每一个人的简介里面的信息\n（性别、\n年龄、\n工作等等）\n都可以用一个向量来表示。所以一个社交网络可以看做是一堆的向量所组成的。\n药物发现（drug discovery）\n跟图有关，如图6.5 所示，一个分子也可以看作是一个图。如\n果把一个分子当做是模型的输入，每一个分子可以看作是一个图，分子上面的每一个球就是\n一个原子，每个原子就是一个向量。每个原子可以用独热向量来表示，比如氢、碳、氧的独热\n向量表示如式(6.2) 所示。\nH = [1, 0, 0, 0, 0, . . . . . . ]\nC = [0, 1, 0, 0, 0, . . . . . . ]\nO = [0, 0, 1, 0, 0, . . . . . . ]\n(6.2)\n如果用独热向量来表示每一个原子，一个分子就是一个图，它就是一堆向量。\n图6.5\n药物发现【需要重绘，版权问题】",
                "metadata": {
                    "chapter": "[6, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/LeeDL_Tutorial_v.1.1.4.pdf",
                    "title": "输入是向量序列的情况"
                }
            },
            {
                "page_content": "附录B 面试题解答\n第1 章面试题解答\n1-1\n强化学习包含环境、动作和奖励3 部分，其本质是智能体通过与环境的交互，使其做出的动作对应的\n决策得到的总奖励最大，或者说是期望最大。\n1-2\n首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构\n建和训练。其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征\n中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个\n距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。另\n外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独\n立同分布条件。\n174→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n1-3\n7 个字总结就是“多序列决策问题”\n，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的\n问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达\n的条件，即满足可学习条件。\n1-4\n深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的\n目的是使总奖励的期望尽可能大。\n1-5\n我认为两者的区别主要在于是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直\n接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环\n境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如\n果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建\n工作。\n第2 章面试题解答\n2-1\n马尔可夫过程是一个二元组< S, P > ，S 为状态集合，P 为状态转移函数；\n马尔可夫决策过程是一个五元组< S, P, A, R, γ >，其中R 表示从S 到S′ 能够获得的奖励期望，γ\n为折扣因子，A 为动作集合；\n马尔可夫最重要的性质是下一个状态只与当前状态有关，与之前的状态无关，也就是p(st+1|st) =\np(st+1|s1, s2, ..., st)。\n2-2\n我们求解马尔可夫决策过程时，可以直接求解贝尔曼方程或动态规划方程：\nV (s) = R(S) + γ\nX\ns′∈S\np(s′|s)V (s′)\n特别地，其矩阵形式为V = R + γPV。但是贝尔曼方程很难求解且计算复杂度较高，所以可以使用动\n态规划、蒙特卡洛以及时序差分等方法求解。\n2-3\n如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势\n必导致决策的泛化能力变差。为了解决这个问题，可以利用循环神经网络对历史信息建模，获得包含历史\n信息的状态表征，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问\n题。\n2-4\n（1）基于状态价值函数的贝尔曼方程: Vπ(s) = P\na π(a|s) P\ns′,r p(s′, r|s, a)[r(s, a) + γVπ(s′)]；\n（2）基于动作价值函数的贝尔曼方程: Qπ(s, a) = P\ns′,r p(s′, r|s, a)[r(s′, a) + γVπ(s′)]。\n2-5\n最佳价值函数的定义为V ∗(s) = maxπ Vπ(s) ，即我们搜索一种策略π 来让每个状态的价值最大。V ∗\n就是到达每一个状态其的最大价值，同时我们得到的策略就可以说是最佳策略，即π∗(s) = arg max\nπ\nVπ(s)\n。最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以说\n某一个马尔可夫决策过程的环境被解。在这种情况下，其最佳价值函数是一致的，即其达到的上限的值是\n一致的，但这里可能有多个最佳策略对应于相同的最佳价值。\n2-6\nn 越大，方差越大，期望偏差越小。价值函数的更新公式如下：\n175\n→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\nQ (S, A) ←Q (S, A) + α\n\" n\nX\ni=1\nγi−1rt+i + γn max\na\nQ (S′, a) −Q (S, A)\n#\n第3 章面试题解答\n3-1\n同策略和异策略的根本区别在于生成样本的策略和参数更新时的策略是否相同。对于同策略，行为策\n略和要优化的策略是同一策略，更新了策略后，就用该策略的最新版本对数据进行采样；对于异策略，其\n使用任意行为策略来对数据进行采样，并利用其更新目标策略。例如，Q 学习在计算下一状态的预期奖励\n时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优的动作，因此这里生成样本\n的策略和学习时的策略不同，所以Q 学习算法是异策略算法；相对应的Sarsa 算法则是基于当前的策略\n直接执行一次动作选择，然后用动作和对应的状态更新当前的策略，因此生成样本的策略和学习时的策略\n相同，所以Sarsa 算法为同策略算法。\n3-2\nQ 学习是通过计算最优动作价值函数来求策略的一种时序差分的学习方法，其更新公式为\nQ(s, a) ←Q(s, a) + α[r(s, a) + γ max\na′\nQ(s′, a′) −Q(s, a)]\n其是异策略的，由于Q 更新使用了下一个时刻的最大值，因此其只关心哪个动作使得Q(st+1, a) 取得最\n大值，而实际上到底采取了哪个动作（行为策略）\n，Q 学习并不关心。这表明优化策略并没有用到行为策\n略的数据，所以说它是异策略的。\n3-3\nSarsa 算法可以算是Q 学习算法的改进，其更新公式为\nQ(s, a) ←Q(s, a) + α[r(s, a) + γQ(s′, a′) −Q(s, a)]\n其为同策略的，Sarsa 算法必须执行两次动作得到(s, a, r, s′, a′) 才可以更新一次；而且a′ 是在特定策略π\n的指导下执行的动作，因此估计出来的Q(s, a) 是在该策略π 下的Q 值，样本生成用的π 和估计的π 是\n同一个，因此是同策略。\n3-4\n（1）生成策略上的差异，前者确定，后者随机。基于价值的方法中动作-价值对的估计值最终会收敛\n（通常是不同的数，可以转化为0～1 的概率）\n，因此通常会获得一个确定的策略；基于策略的方法不会收\n敛到一个确定的值，另外他们会趋向于生成最佳随机策略。如果最佳策略是确定的，那么最优动作对应的\n值函数的值将远大于次优动作对应的值函数的值，值函数的大小代表概率的大小。\n（2）动作空间是否连续，前者离散，后者连续。基于价值的方法，对于连续动作空间问题，虽然可以\n将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优动作，会在\n最优动作附近徘徊；过小的离散间距会使得动作的维度增大，会和高维度动作空间一样导致维度灾难，影\n响算法的速度。而基于策略的方法适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作\n的概率，而是通过正态分布选择动作。\n（3）基于价值的方法，例如Q 学习算法，是通过求解最优价值函数而间接地求解最优策略；基于策略\n的方法，例如REINFORCE 等算法直接将策略参数化，通过策略搜索、策略梯度或者进化方法来更新参\n数以最大化回报。基于价值的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举等策略时\n会有收敛问题。策略梯度具有良好的收敛性。\n（4）另外，对于价值迭代和策略迭代，策略迭代有两个循环，一个是在策略估计的时候，为了求当前\n策略的价值函数需要迭代很多次；另一个是外面的大循环，即策略评估、策略提升。价值迭代算法则是一\n步到位，直接估计最优价值函数，因此没有策略提升环节。\n176\n→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n3-5\n时序差分算法是使用广义策略迭代来更新Q 函数的方法，核心是使用自举，即价值函数的更新使用\n下一个状态的价值函数来估计当前状态的价值。也就是使用下一步的Q 值Q(st+1, at+1) 来更新当前步的\nQ 值Q(st, at)。完整的计算公式如下：\nQ(st, at) ←Q(st, at) + α[rt+1 + γQ(st+1, at+1)]\n3-6\n蒙特卡洛方法是无偏估计，时序差分方法是有偏估计；蒙特卡洛方法的方差较大，时序差分方法的方\n差较小，\n原因在于时序差分方法中使用了自举，\n实现了基于平滑的效果，\n导致估计的价值函数的方差更小。\n3-7\n相同点：\n都用于进行价值函数的描述与更新，\n并且所有方法都基于对未来事件的展望计算一个回溯值。\n不同点：蒙特卡洛方法和时序差分方法属于免模型方法，而动态规划属于有模型方法；时序差分方法\n和蒙特卡洛方法，因为都是免模型的方法，所以对于后续状态的获知也都是基于试验的方法；时序差分方\n法和动态规划方法的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的价值函数的更\n新。\n另外，时序差分方法不需要等到试验结束后才能进行当前状态的价值函数的计算与更新，而蒙特卡洛\n方法需要与环境交互，产生一整条马尔可夫链并直到最终状态才能进行更新。时序差分方法和动态规划方\n法的策略评估不同之处为免模型和有模型，动态规划方法可以凭借已知转移概率推断出后续的状态情况，\n而时序差分方法借助试验才能知道。\n蒙特卡洛方法和时序差分方法的不同在于，蒙特卡洛方法进行了完整的采样来获取长期的回报值，因\n而在价值估计上会有更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于其\n基于试验的采样得到，和真实的分布有差距，不充足的交互导致较大方差。而时序差分方法则相反，因为\n它只考虑了前一步的回报值，其他都是基于之前的估计值，因而其价值估计相对来说具有偏差大方差小的\n特点。\n三者的联系：对于TD(λ) 方法，如果λ = 0 ，那么此时等价于时序差分方法，即只考虑下一个状态；\n如果λ = 1 ，等价于蒙特卡洛方法，即考虑T −1 个后续状态直到整个试验结束。\n第4 章面试题解答\n4-1\n首先我们的目的是最大化奖励函数，即调整θ ，使得期望回报最大，可以用公式表示如下：\nJ(θ) = Eτ∼pθ(τ)\n\"X\nt\nr(st, at)\n#\n其中τ 表示从开始到结束的一条完整轨迹。通常对于最大化问题，我们可以使用梯度上升算法找到最\n大值，即\nθ∗= θ + α∇J(θ)\n所以我们仅仅需要计算并更新∇J(θ) ，也就是计算奖励函数J(θ) 关于θ 的梯度，也就是策略梯度，\n计算方法如下：\n∇θJ(θ) =\nZ\n∇θpθ(τ)r(τ)dτ =\nZ\npθ∇θlogpθ(τ)r(τ)dτ = Eτ∼pθ(τ)[∇θlogpθ(τ)r(τ)]\n接着我们继续展开，对于pθ(τ) ，即pθ(τ|θ) ：\n177\n→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\npθ(τ|θ) = p(s1)\nT\nY\nt=1\nπθ(at|st)p(st+1|st, at)\n取对数后为：\nlogpθ(τ|θ) = logp(s1) +\nT\nX\nt=1\nlogπθ(at|st)p(st+1|st, at)\n继续求导：\n∇logpθ(τ|θ) =\nT\nX\nt=1\n∇θlogπθ(at|st)\n代入第3 个式子，可以将其化简为：\n∇θJ(θ) = Eτ∼pθ(τ)[∇θlogpθ(τ)r(τ)]\n= Eτ∼pθ[(∇θlogπθ(at|st))(\nT\nX\nt=1\nr(st, at))]\n= 1\nN\nN\nX\ni=1\n[(\nT\nX\nt=1\n∇θlogπθ(ai,t|si,t))(\nN\nX\nt=1\nr(si,t, ai,t))]\n4-2\n（1）增加基线：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一个变换\n的概率上升，我们把奖励减去一项b，称b 为基线。当减去b 以后，就可以让奖励R(τ n) −b 有正有负。\n如果得到的总奖励R(τ n) 大于b ，就让它的概率上升。如果总奖励小于b，就算它是正的，值很小也是不\n好的，就需要让它的概率下降。如果总奖励小于b ，就要让采取这个动作的奖励下降，这样也符合常理。\n但是使用基线会让本来奖励很大的“动作”的奖励变小，降低更新速率。\n（2）指派合适的分数：首先，原始权重是整个回合的总奖励。现在改成从某个时间点t 开始，假设这\n个动作是在时间点t 被执行的，那么从时间点t ，一直到游戏结束所有奖励的总和，才真的代表这个动作\n是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，这里我们称由此得到的奖励的和为\n折扣回报。\n（3）综合以上两种技巧，我们将其统称为优势函数，用A 来代表优势函数。优势函数取决于状态和动\n作，即我们需计算的是在某一个状态s 采取某一个动作a 的时候，优势函数有多大。\n第5 章面试题解答\n5-1\n使用另外一种分布，来逼近所求分布的一种方法，算是一种期望修正的方法，公式如下：\nZ\nf(x)p(x)dx =\nZ\nf(x)p(x)\nq(x)q(x)dx = Ex∼q[f(x)p(x)\nq(x)] = Ex∼p[f(x)]\n我们在已知q 的分布后，可以使用上式计算出从p 分布的期望值。也就可以使用q 来对p 进行采样\n了，即重要性采样。\n5-2\n我可以用一句话概括两者的区别，即生成样本的策略（价值函数）和网络参数更新时的策略（价值函\n数）\n是否相同。\n具体来说，\n同策略, 生成样本的策略\n（价值函数）\n与网络更新参数时使用的策略\n（价值函数）\n相同。Sarsa 算法就是同策略的，其基于当前的策略直接执行一次动作，然后用价值函数的值更新当前的\n策略，因此生成样本的策略和学习时的策略相同，算法为同策略算法。该算法会遭遇探索-利用窘境，仅利\n用目前已知的最优选择，可能学不到最优解，不能收敛到局部最优，而加入探索又降低了学习效率。ε-贪\n178\n→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n心算法是这种矛盾下的折中，其优点是直接了当、速度快，缺点是不一定能够找到最优策略。异策略，生\n成样本的策略（价值函数）与网络更新参数时使用的策略（价值函数）不同。例如，Q 学习算法在计算下\n一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优动作，因\n此这里生成样本的策略和学习时的策略不同，即异策略算法。\n5-3\n近端策略优化算法借鉴了信任区域策略优化算法，通过采用一阶优化，在采样效率、算法表现以及实\n现和调试的复杂度之间取得了新的平衡。这是因为近端策略优化算法会在每一次迭代中尝试计算新的策\n略，让损失函数最小化，并且保证每一次新计算出的策略能够和原策略相差不大。换句话说，其为在避免\n使用重要性采样时由于在θ 下的pθ (at|st) 与在θ′ 下的pθ′ (at|st) 差太多，导致重要性采样结果偏差较大\n而采取的算法。\n第6 章面试题解答\n6-1\n深度Q 网络是基于深度学习的Q 学习算法，其结合了价值函数近似与神经网络技术，并采用了目标\n网络和经验回放技巧进行网络的训练。\n6-2\n在深度Q 网络中某个动作价值函数的更新依赖于其他动作价值函数。如果我们一直更新价值网络的\n参数，会导致更新目标不断变化，也就是我们在追逐一个不断变化的目标，这样势必会不太稳定。为了解\n决基于时序差分的网络中，优化目标Qπ (st, at) = rt + Qπ (st+1, π (st+1)) 左右两侧会同时变化使得训练过\n程不稳定，从而增大回归难度的问题，目标网络选择将优化目标的右边即rt + Qπ (st+1, π (st+1)) 固定，通\n过改变优化目标左边的网络参数进行回归。对于经验回放，其会构建一个回放缓冲区，用来保存许多数据，\n每一个数据的内容包括：状态st、采取的动作at、得到的奖励rt、下一个状态st+1。我们使用π 与环境\n交互多次，把收集到的数据都放到回放缓冲区中。当回放缓冲区“装满”后，就会自动删去最早进入缓冲\n区的数据。在训练时，对于每一轮迭代都有相对应的批量（与我们训练普通网络一样，通过采样得到）\n，然\n后用这个批量中的数据去更新Q 函数。即Q 函数在采样和训练的时候会用到过去的经验数据，也可以消\n除样本之间的相关性。\n6-3\n整体来说，从名称就可以看出，两者的目标价值以及价值的更新方式基本相同。但有如下不同点：\n（1）首先，深度Q 网络将Q 学习与深度学习结合，用深度网络来近似动作价值函数，而Q 学习则是\n采用表格进行存储。\n（2）深度Q 网络采用了经验回放的技巧，从历史数据中随机采样，而Q 学习直接采用下一个状态的\n数据进行学习。\n6-4\n随机性策略表示为某个状态下动作取值的分布，确定性策略在每个状态只有一个确定的动作可以选。\n从熵的角度来说，确定性策略的熵为0，没有任何随机性。随机性策略有利于我们进行适度的探索，确定\n性策略不利于进行探索。\n6-5\n在神经网络中通常使用随机梯度下降法。随机的意思是我们随机选择一些样本来增量式地估计梯度，\n比如常用的批量训练方法。如果样本是相关的，就意味着前后两个批量很可能也是相关的，那么估计的梯\n度也会呈现出某种相关性。但是在极端条件下，后面的梯度估计可能会抵消掉前面的梯度估计量，从而使\n得训练难以收敛。\n第7 章面试题解答\n7-1\n179\n→_→\n欢迎去各大电商平台选购纸质版蘑菇书《Easy RL：强化学习教程》\n←_←\n评论员衡量当前决策的好坏。结合策略模块，当评论员判别某个动作的选择是有益的时候，策略就更\n新参数以增大该动作出现的概率，反之减小该动作出现的概率。\n9-6\n优势函数的计算公式为A(s, a) = Q(s, a) −V (s) = r + γV (s′) −V (s) ，其可以定量地表示选择动作\na 的优势。即当动作a 低于价值函数的平均值的时候，优势函数为负值；反之为正值。其是一个标量，具\n体来说：\n（1）如果A(s, a) > 0 ，梯度被推向正方向；\n（2）如果A(s, a) < 0 ，即我们的动作比该状态下的平均值还差，则梯度被推向反方向。\n这样就需要两个价值函数，所以可以使用时序差分方法做误差估计：A(s, a) = r + γV (s′) −V (s) 。\n第12 章面试题解答\n12-1\n深度确定性策略梯度算法使用演员-评论员结构，但是输出的不是动作的概率，而是具体动作，其可以\n用于连续动作的预测。优化的目的是将深度Q 网络扩展到连续的动作空间。另外，其含义如其名：\n（1）深度是因为用了深度神经网络；\n（2）确定性表示其输出的是一个确定的动作，可以用于连续动作的环境；\n（3）策略梯度代表的是它用到的是策略网络。强化算法每个回合就会更新一次网络，但是深度确定性\n策略梯度算法每个步骤都会更新一次策略网络，它是一个单步更新的策略网络。\n12-2\n异策略算法。\n（1）深度确定性策略梯度算法是优化的深度Q 网络，其使用了经验回放，所以为异策略\n算法。\n（2）因为深度确定性策略梯度算法为了保证一定的探索，对输出动作加了一定的噪声，行为策略不\n再是优化的策略。\n12-3\n分布的分布式深度确定性策略梯度算法\n（distributed distributional deep deterministic policy gradient，\nD4PG)，相对于深度确定性策略梯度算法，其优化部分如下。\n（1）分布式评论员：不再只估计Q 值的期望值，而是估计期望Q 值的分布，即将期望Q 值作为一个\n随机变量来估计。\n（2）N 步累计回报：计算时序差分误差时，D4PG 计算的是N 步的时序差分目标值而不仅仅只有一\n步，这样就可以考虑未来更多步骤的回报。\n（3）多个分布式并行演员：D4PG 使用K 个独立的演员并行收集训练数据并存储到同一个回放缓冲\n区中。\n（4）优先经验回放（prioritized experience replay，PER）\n：使用一个非均匀概率从回放缓冲区中进行\n数据采样。\n181",
                "metadata": {
                    "chapter": "[15]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "附录B面试题解答"
                }
            },
            {
                "page_content": "12.6\n习题\n12-1 请解释随机性策略和确定性策略，两者有什么区别？\n12-2 对于连续动作的控制空间和离散动作的控制空间，如果我们都采取策略网络，应该分别如何操\n作？",
                "metadata": {
                    "chapter": "[12, 6]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "习题"
                }
            },
            {
                "page_content": "4.3\n剪枝处理\n本节内容通俗易懂，跟着“西瓜书”中的例子动手演算即可，无需做过多解释。以下仅结合图4.5 继\n续讨论一下图4.2 中的递归返回条件。图4.5 与图4.4 均是基于信息增益生成的决策树，不同在于图4.4\n基于表4.1，而图4.5 基于表4.2 的训练集。\n结点3 包含训练集“脐部”为稍凹的样本（编号6、7、15、17）\n，当根据“根蒂”再次进行划分时不\n含有“根蒂”为硬挺的样本（递归返回情形(3)）\n，而恰巧四个样本（编号6、7、15、17）含两个好瓜和两\n个坏瓜，因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。\n结点5 包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本（编号6、7、15）\n，当根据“色泽”再次\n进行划分时不含有“色泽”为浅白的样本（递归返回情形(3)）\n，因此叶结点浅白类别标记为好瓜（编号6、\n7、15 样本中，前两个为好瓜，最后一个为坏瓜）\n。\n结点6 包含训练集“脐部”为稍凹、\n“根蒂”为稍蜷、\n“色泽”为乌黑的样本（编号7、15）\n，当根据“纹\n理”再次进行划分时不含有“纹理”为模糊的样本（递归返回情形(3)）\n，而恰巧两个样本（编号7、15）含\n好瓜和坏瓜各一个，因此叶结点模糊的类别随机从类别好瓜和坏瓜中选择其一。\n图4.5 两次随机选择均选为好瓜，实际上表示了一种归纳偏好（参见第1 章1.4 节）\n。",
                "metadata": {
                    "chapter": "[4, 3]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "剪枝处理"
                }
            },
            {
                "page_content": "3.6\n类别不平衡问题\n对于类别不平衡问题，\n“西瓜书”2.3.1 节中的“精度”通常无法满足该特殊任务的需求，例如“西瓜\n书”在本节第一段的举例：有998 个反例和2 个正例，若机器学习算法返回一个永远将新样本预测为反\n例的学习器则能达到99.8% 的精度，显然虚高，因此在类别不平衡时常采用2.3 节中的查准率、查全率和\nF1 来度量学习器的性能。\n参考文献\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\n[2] Wikipedia contributors. Matrix calculus, 2022.\n[3] 张贤达. 矩阵分析与应用. 第2 版. 清华大学出版社, 2013.\n→_→\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n←_←→_→\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\n←_←\n第4 章\n决策树",
                "metadata": {
                    "chapter": "[3, 6]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "类别不平衡问题"
                }
            },
            {
                "page_content": "第6 章\n支持向量机\n在深度学习流行之前，支持向量机及其核方法一直是机器学习领域中的主流算法，尤其是核方法至今\n都仍有相关学者在持续研究。",
                "metadata": {
                    "chapter": "[6]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "支持向量机"
                }
            },
            {
                "page_content": "6.1.1\n图6.1 的解释\n回顾第5 章5.2 节的感知机模型可知，图6.1 中的黑色直线均可作为感知机模型的解，因为感知机模\n型求解的是能将正负样本完全正确划分的超平面，因此解不唯一。而支持向量机想要求解的则是离正负样\n本都尽可能远且刚好位于“正中间”的划分超平面，因为这样的超平面理论上泛化性能更好。",
                "metadata": {
                    "chapter": "[6, 1, 1]",
                    "source": "/home/lu/workspace/public/datawhalechina/pumpkin_book.pdf",
                    "title": "图6.1的解释"
                }
            },
            {
                "page_content": "1.2\n序列决策",
                "metadata": {
                    "chapter": "[1, 2]",
                    "source": "/home/lu/workspace/public/datawhalechina/EasyRL_v1.0.6.pdf",
                    "title": "序列决策"
                }
            }
        ]
    }
]